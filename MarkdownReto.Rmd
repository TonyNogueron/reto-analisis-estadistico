---
title: "Análisis"
author: "Paulina Galindo, Antonio Noguerón, Pamela Ruíz"
date: "6/1/2022"
output: html_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(readr)
library(skimr)
library(readxl)
library(ggplot2)
library(corrplot)
library(GGally)
library(psych)
library(Hmisc)
library(car)
library(olsrr)
library(nortest)
library(lmtest)
library(broom)
library(forecast)



historico <- read_excel("Datos_Situacion_Problema.xlsx", sheet="Histórico de defectos")
muestreo <- read_excel("Datos_Situacion_Problema.xlsx", sheet="Datos de muestreo", skip=1)
indice <- read_excel("Datos_Situacion_Problema.xlsx", sheet="Índice")

head(muestreo)
str(muestreo)
resumen <- summary(muestreo)
colnames <- names(muestreo)
num_datos <- nrow(muestreo)
```

# Introducción a la situación problema

Usted trabaja en una empresa que se dedica a la producción de piezas automotrices a base de Polipropileno. Debido a cuestiones personales, el técnico experto en el proceso de extrusión salió de la empresa y el porcentaje de defectos está por los cielos.

Es necesario determinar las variables importantes para el proceso de extrusión de Polipropileno en una máquina de tornillo sencillo para poder mejorar la producción. Es de particular interés el porcentaje de producto defectuoso. Cada producto es inspeccionado visualmente por personal del departamento de calidad. Para esto, cada cierto tiempo se toman muestras de producto final y se determina si éste tiene algún defecto. Una vez inspeccionado cada producto, se clasifica como defectuoso o no defectuoso y se calcula un cociente entre los defectuosos y el total producido para determinar el porcentaje de producto defectuoso.

La máquina de extrusión que se utiliza en el proceso es una Welex y de acuerdo con su información técnica, hay *9 variables que podrían afectar la calidad del producto*, tal y como se muestra en la tabla siguiente:
  
  | FACTOR | NOMBRE DE LA VARIABLE |
  | :----: | :----: |
  |A|Presión bomba|
  |B|Temperatura plástico 3(bomba)|
  |C|Temperatura plástico 4(mezcladora)|
  |D|Temperatura tornillo (usillo)|
  |E|RPM tornillo (usillo)|
  |F|Temperatura barril|
  |G|Velocidad extrusión|
  |H|Temperatura enfriadores|
  |I|Tipo materia prima|
  
  ## Para empezar nuestro análisis renombramos las variables
  
```{r}
attach(muestreo)
#        NUEVO                 # ANTERIOR
datos<- rename(muestreo, PresionBomba = A,
               TempPlastico3Bomba  = B,
               TempPlastico4Mezcladora = C,
               TempTornilloUsillo = D,
               RpmTornilloUsillo = E,
               TempBarril = F,
               VelocidadExtrusion = G,
               TempEnfriadores = H,
               TipoMateria = I,
               PorcentajeDefectos = Y
)

names(datos)
attach(datos)

```
## Seguido de esto graficamos cada variable respecto al porcentaje de error
```{r}
plot(A, Y, main="Gráfica presión v.s. porcentaje de defectos", xlab="Presión", ylab="Defectos %")
plot(B, Y, main="Gráfica temp plástico 3 v.s. porcentaje de defectos", xlab="Temp plástico 3", ylab="Defectos %")
plot(C, Y, main="Gráfica temp plástico 4 v.s. porcentaje de defectos", xlab="Temp plástico 4", ylab="Defectos %")
plot(D, Y, main="Gráfica temp tornillo v.s. porcentaje de defectos", xlab="Temp tornillo", ylab="Defectos %")
plot(E, Y, main="Gráfica RPM v.s. porcentaje de defectos", xlab="RPM", ylab="Defectos %")
plot(F, Y, main="Gráfica temp barril v.s. porcentaje de defectos", xlab="Temp barril", ylab="Defectos %")
plot(G, Y, main="Gráfica velocidad v.s. porcentaje de defectos", xlab="Velocidad", ylab="Defectos %")
plot(H, Y, main="Gráfica temp enfriadores v.s. porcentaje de defectos", xlab="Temp enfriadores", ylab="Defectos %")
plot(I, Y, main="Gráfica materia prima v.s. porcentaje de defectos", xlab="Materia prima", ylab="Defectos %")

```

## Creamos el histograma

Para encontrar el mejor modelo de regresión lineal, primero debemos determinar si la distribución de los datos en cada variable es normal. Esta distribución nos ayudará a comprobar la linealidad y la validez de usar estos datos en el modelo.

Para observar la distribución de los datos graficaremos cada variable excepto la variable I que representa el tipo de material porque esta es una variable cualitativa que representa los 3 tipos de materiales.
Gracias a que esta variable es cualitativa, nuestros datos se dividen en 3 grupos, uno por cada material.


```{r}
muestreo2 <- select(muestreo, -I, -Y)
multi.hist(x = muestreo2, dcol = c("blue", "red"),
           dlty = c("dotted","solid"), lwd = c(2,1),
           main = c("A", "B", "C", "D", "E", "F", "G", "H"))
```


## Después obtuvimos el correlograma para poder obtener un análisis más visual de la  correlación entre las variables.
Al ver el correlograma, vimos que la velocidad de extrusión y la Presión de la bomba están relacionadas.
```{r}
corrplot(cor(datos))
```
## Creamos la gráfica de pares: 

```{r}
ggpairs(datos)

```

Con estas gráficas podemos observar que existe una correlación de 0.859 entre la variable de Presión de la Bomba y la Velocidad de Extrusión. Esto significa que tienen una relación directamente proporcional.


## Graficamos las variables entre ellas

```{r}
plot(PresionBomba, VelocidadExtrusion, main="Gráfica Presión de la Bomba \n v.s. Velocidad de extrusión", xlab="Presión de la Bomba [Bar]", ylab="Velocidad Extrusión [m/s]")
plot(TempPlastico4Mezcladora, PorcentajeDefectos, main="Gráfica de Temperatura de Plástico 4 (mezcladora) \n v.s. Porcentaje de Defectos", xlab="Temperatura Plástico 4 [°C]", ylab="Defectos [%]")
plot(TempPlastico3Bomba, PorcentajeDefectos, main="Gráfica de Temperatura de Plástico 3 (bomba) \n v.s. Porcentaje de Defectos", xlab="Temperatura Plástico 3 [°C]", ylab="Defectos [%]")
plot(TempBarril, PorcentajeDefectos, main="Gráfica de Temperatura del Barril \n v.s. Porcentaje de Defectos", xlab="Temperatura del Barril [°C]", ylab="Defectos [%]")
```

## Obtuvimos la regresión lineal simple con una de las variables con mayor correlación

```{r}
regresionSimple <- lm(Y ~ C, data=muestreo)
summary(regresionSimple)
```

## Analizamos la regresión lineal múltiple:

Para encontrar el mejor modelo de regresión lineal haremos varios modelos en donde haremos pruebas de hipótesis para ir eliminando las variables que no son significativas.

Primero haremos un modelo de regresión para todo el conjunto de datos, pero como hay 3 tipos de materiales distintos, dividiremos nuestro conjunto de datos en 3 y realizaremos un modelo de regresión múltiple para cada conjunto de datos filtrado por material.


### Regresión lineal de todas las variables
```{r}
regresion <- lm(Y ~ A + B + C + D + E + F + G + H + I, data=muestreo)
summary(regresion)
```
El modelo con todas las variables introducidas como predictores, tiene un R de 0.7237, lo que nos indica que es capaz de explicar el 72.37% de la variabilidad observada en el porcentaje de defectos.
Después de hacer las pruebas de hipótesis podemos concluir que en las variables G (velocidad extrusión), H (temperatura enfriadores), I (tipo de material) no se rechaza la hipótesis nula; por ende, se eliminarán del modelo.


Además, es necesario eliminar la variable G (velocidad extrusión) porque tiene un coeficiente de correlación de 0.859 con la variable A (presión bomba). Esto significa que están fuertemente relacionadas y para que un modelo de regresión múltiple sea correcto, es necesario que las variables sean independientes entre si; si no, una de las variables debe eliminarse.

## Regresión lineal con las variables que tiene un valor p < 0.05
#### Regresión lineal eliminando:
- Velocidad de extrusión
- Temperatura de los enfriadores
- Tipo de material

```{r}
regresion1 <- lm(Y ~ A + B + C + D + E + F, data=muestreo)
summary(regresion1)
```

El modelo sin las variables mencionadas anteriormente, tiene un R de 0.7293, lo que nos indica que es capaz de explicar el 72.93% de la variabilidad observada en el porcentaje de defectos. Después de realizar las pruebas de hipótesis podemos concluir que en todas las variables se rechaza la hipótesis nula y este es el mejor modelo.

Para poder confirmar que el modelo escogido es el mejor, usamos la función step de R Studio que realiza todos los modelos de regresión múltiple, hace la comparación del valor AIC y regresa la fórmula del mejor modelo.


# Comprobación del modelo de regresión múltiple

```{r}
step(object = regresion, direction = "both", trace = 1)

```

Elegimos el modelo con menor AIC.

```{r}
 bestLm <- lm(formula = Y ~ A + B + C + D + E + F, data = muestreo)

summary(bestLm)

```


## Validación del modelo elegido

El modelo tiene un R^2 de 0.7293, por lo que es capaz de explicar el 72.93% de la variabilidad observada en el porcentaje de defectos.

Para que un modelo de regresión múltiple sea considerado "correcto" debe cumplir con los siguientes supuestos:

1. Linealidad
2. Independencia de los residuos
3. Homoscedasticidad
4. Normalidad en los residuos

Además, debe haber más observaciones en el conjunto de datos que parámetros y las variables deben ser independientes entre ellas.

```{r}
plot(bestLm)
```

### Análisis de la gráfica residuos vs valores ajustados

En esta gráfica lo ideal para poder concluir que el supuesto de linealidad se cumple es que la línea roja debe estar lo más horizontal posible y los datos deben estar dispersos. Como esto sí se cumple, se cumple el supuesto de linealidad.

Los datos 91, 45 y 13 presentan diferencias mayores, por lo que podrían tener algún problema.

### Análisis de la gráfica normalidad de residuos

En esta gráfica lo ideal para poder concluir que el supuesto de normalidad en los residuos se cumple es que la mayoría de los datos deben estar sobre la línea punteada. Como esto sí se cumple, podemos concluir que sí hay normalidad en los residuos.

Los datos 91, 45 y 13 se alejan de la normalidad.


### Análisis de la gráfica localización-escala

En esta gráfica lo ideal para poder concluir que el supuesto de homoscedasticidad (varianza constante) se cumple es que la línea roja debe ser lo más horizontal posible y los datos deben estar dispersos. Como esto no se cumple totalmente, no podemos concluir que hay homoscedasticidad.

Los datos 91, 45 y 13 al aparecer por tercera vez nos indican que deberíamos analizar con más detalle para ubicar un posible problema.


### Análisis de la gráfica residuos vs leverage

En esta gráfica podemos determinar los datos atípicos de nuestra base de datos. Los datos que estén fuera de los límites marcados por la línea punteada roja (la distancia de Cook) son los datos atípicos.

Como todos los residuos se encuentran dentro de la distancia de Cook, ningún dato es atípico.



## Confirmación de Supuestos

Con las interpretaciones de las gráficas anteriores, concluimos que se cumple el supuesto de linealidad y normalidad. Para poder confirmar esto, podemos realizar ciertas pruebas.

### Prueba Durbin-Watson

La prueba Drubin-Watson nos sirve para verificar si hay una independencia en nuestros datos.
Si el valor-p es mayor que alfa, no se rechaza la hipótesis nula y concluimos que sí hay independencia.
Si el valor-p es menor que alfa, sí se rechaza la hipótesis nula y concluimos que no hay independencia.  

```{r}
dwtest(bestLm, data = muestreo)
```
Podemos observar que el valor-p es de 0.8844; por ende, sí se cumple el supuesto de independencia.

### Prueba Breusch-Pagan

La prueba Breusch-Pagan es una prueba que nos permite verificar si existe la homocedasticidad (varianza constante).

Si el valor-p es mayor que alfa, no se rechaza la hipótesis nula y concluimos que sí tienen varianza constante.
Si el valor-p es menor que alfa, sí se rechaza la hipótesis nula y concluimos que no tienen varianza constante. 


```{r}
bptest(bestLm)
```
Podemos observar que el valor-p es de 0.1777; por ende, nuestros residuos sí tienen varianza constante.


### Prueba Lillie

La prueba lillie nos sirve para verificar si existe la normalidad en los residuos.
Primero se grafica un histograma y se determina si tiene una distribución normal.
Después para verificar esta normalidad de forma numérica se hace la prueba Lillie.
Si el valor-p es mayor que alfa, no se rechaza la hipótesis nula y concluimos que sí hay normalidad en los residuos.
Si el valor-p es menor que alfa, sí se rechaza la hipótesis nula y concluimos que no hay normalidad en los residuos.


```{r}
hist(bestLm$residuals)
lillie.test(bestLm$residuals)

```

Podemos observar que el histograma sí se ve distribuido y como el valor-p es de 0.2745, concluimos que sí hay normalidad en los residuos.


## Modelo de predicción

Logramos obtener un buen modelo de regresión múltiple que cumple con los supuestos y validaciones, ahora para ayudar a la empresa, iremos variando algunos datos para encontrar las mejores configuraciones para obtener un porcentaje de defectos bajo.

Realizaremos un breve análisis estadístico de los datos para encontrar posibles valores para las variables. Usaremos la función describe().


```{r}
#Datos escogidos aleatoriamente
datos_nuevos <- data.frame(A=73,B=213,C=220,D=195,E=110,F=130,G=4.5,H=70,I=2)
predict(bestLm, datos_nuevos)

descripcion <- describe(muestreo)
descripcion
```

Tomando las demás variables constantes con el valor de sus medias, veremos como va influyendo el cambio de valores en una variable independiente para ir guardando las posibles configuraciones.

Iniciaremos con la variable A(presión de la bomba): Esta tiene un valor mínimo de 16 y máximo de 75.

```{r}
#Dejando las demás variables con el valor de sus medias, haremos una predicción del porcentaje de defectos con presión de la bomba:

#baja
datos_nuevos <- data.frame(A=20,B=221.7,C=214.5,D=220.3,E=98.51,F=146.7)
predict(bestLm, datos_nuevos)

#media
datos_nuevos <- data.frame(A=45,B=221.7,C=214.5,D=220.3,E=98.51,F=146.7)
predict(bestLm, datos_nuevos)

#alta
datos_nuevos <- data.frame(A=75,B=221.7,C=214.5,D=220.3,E=98.51,F=146.7)
predict(bestLm, datos_nuevos)
```

Podemos ver después de este breve análisis que una presión de la bomba alta, disminuye el porcentaje de defectos

Ahora realizaremos este proceso con las demás variables y daremos un breve resumen de la información obtenida.

## Información obtenida con pruebas de predicción.

#### Variable A: Presión de la Bomba [Bar]:
  Después de las pruebas pudimos obtener que a mayor presión, menor porcentaje de defectos.

#### Variable B: Temperatura del Plástico 3 (Bomba) [°C]:
  A mayor temperatura, menor porcentaje de defectos.

#### Variable C: Temperatura del Plástico 4 (Mezcladora) [°C]:
  A mayor temperatura, menor porcentaje de defectos.

#### Variable D: Temperatura tornillo (Usillo) [°C]:
  A menor temperatura, menor porcentaje de defectos.

#### Variable E: Velocidad del tornillo (usillo) [rmp]:
  A mayor velocidad, menor porcentaje de defectos.

#### Variable F: Temperatura del Barril [°C]:
  A mayor temperatura, menor porcentaje de defectos.

## Configuraciones sugeridas para un Porcentaje de defectos bajo:

Después de realizar las pruebas y analizar el comportamiento del porcentaje de defectos con respecto a las demás variables, logramos predecir un porcentaje de defectos bajo. Esto lo logramos proponiendo valores para las máquinas que estuvieran dentro del rango de los datos dados por la empresa.

```{r}
datos_nuevos <- data.frame(A=68,B=239.2,C=249.3, D=186,E=151.2,F=178)
predict(bestLm, datos_nuevos)
```

### Configuraciones usadas para Porcentaje de defectos de: 3.03%

- Presión de la Bomba [Bar]: 68
- Temperatura del Plástico 3 (Bomba) [°C]: 239.2
- Temperatura del Plástico 4 (Mezcladora) [°C]: 249.3
- Temperatura tornillo (Usillo) [°C]: 186
- Velocidad del tornillo (usillo) [rmp]: 151.2
- Temperatura del Barril [°C]: 178


## Modelos de regresión múltiple por cada tipo de material

Para poder crear tres modelos de regresión múltiple para cada tipo de material, debemos filtrar los datos originales para crear tres dataframes que representen los datos obtenidos al usar cierto tipo de material.


```{r}
tipo1 <- filter(muestreo, I==1)
tipo2 <- filter(muestreo, I==2)
tipo3 <- filter(muestreo, I==3)
```


## Dataframe con tipo de material 1
```{r}
tipo1
```

### Dataframe con tipo de material 2
```{r}
tipo2
```

### Dataframe con tipo de material 3
```{r}
tipo3
```

Después de crear los tres dataframes filtrados, creamos el modelo inicial de cada uno utilizando todas las variables y realizamos las pruebas de hipótesis hasta encontrar el mejor modelo posible.


### Modelo para el tipo de material 1
```{r}
m1 = lm(Y~A+B+C+D+E+F+G+H, data=tipo1)
summary(m1)
step(object = m1, direction = "both", trace = 1)
m1 = lm(formula = Y ~ B + C + D + E + F, data = tipo1)
summary(m1)
```


### Modelo para el tipo de material 2
```{r}
m2 = lm(Y~A+B+C+D+E+F+G+H, data=tipo2)
summary(m2)
step(object = m2, direction = "both", trace = 1)
m2 = lm(formula = Y ~ B + C + D + E + F, data= tipo2)
summary(m2)
```

### Modelo para el tipo de material 3
``` {r}
m3 = lm(Y~A+B+C+D+E+F+G+H, data=tipo3)
summary(m3)
step(object = m3, direction = "both", trace = 1)
m3 = lm(formula = Y ~ B + C + D + E + F, data= tipo3)
summary(m3)

```


# Análisis de observaciones atípicas

```{r}
ols_plot_cooksd_bar(bestLm)

```



----------------------------------------------------------------------------
# REVISAR!!!!!!






# Multicolinealidad

```{r}
rcorr(as.matrix(muestreo))
```

```{r}
corrplot(cor(muestreo))
```


## Tolerancia  y Factor de Inflación de la Varianza del mejor modelo

```{r}
vif(bestLm)

```

```{r}
tol <- 1/vif(bestLm)
tol
```

Las covariables están relacionadas entre sí y su tolerancia no está debajo de 0.2 no indica que haya un problema serio.

# Identificación de valores atípicos o influyentes en la regresión

Análisis de las variables por separado.

```{r}
boxplot.matrix(as.matrix(muestreo),use.cols = T)
```


# Detección de influyentes
*** revisar
```{r}
i <- nrow(bestLm)
abs(dfbeta(bestLm))>sqrt(2/i)
```

Identificación de observaciones que son potencialmente influyentes.

```{r}
summary(influence.measures(bestLm))

```
```{r}
influencePlot(bestLm)
```

Los círculos nos ayudan a detectar las observaciones influyentes y el tamaño de los círculos denota la intensidad de su influencia.

Analizando la gráfica podemos observar que los datos 91, 30, 40 y 13 tienen mayor influencia que el resto de estos; además al tener un mayor tamaño son datos a los que debemos tener mayor cuidado.



# Análisis de serie de tiempo

Datos mensuales de las piezas defectuosas (2 años).

  | PRODUCTO | PIEZAS PRODUCIDAS AL MES |
  | :----: | :----: |
  |A| 300,000 |
  |B| 40,000 |
  |C| 200,000 |
  

```{r include=FALSE}
historicoA <- read_excel("Datos_Situacion_Problema.xlsx", 
                                       sheet = "Producto A", col_types = c("numeric", 
                                                                           "numeric", "numeric"), skip = 2)

historicoB <- read_excel("Datos_Situacion_Problema.xlsx", 
                         sheet = "Producto B", col_types = c("numeric", 
                                                             "numeric", "numeric"), skip = 2)

historicoC <- read_excel("Datos_Situacion_Problema.xlsx", 
                         sheet = "Producto C", col_types = c("numeric", 
                                                             "numeric", "numeric"), skip = 2)

# convertimos data frame a serie de tiempo
productoA = ts(historicoA$`defectuosas`, start = 1, frequency = 12); #plot(productoA)
productoB = ts(historicoB$`defectuosas`, start = 1, frequency = 12); #productoB
productoC = ts(historicoC$`defectuosas`, start = 1, frequency = 12); #productoC

```

## Comparación de la distribución de los productos defectuosos cada mes

```{r echo=FALSE}
boxplot(productoA~ cycle(productoA),main="Producto A")
boxplot(productoB~ cycle(productoA),main="Producto B")
boxplot(productoC~ cycle(productoA),main="Producto C")
#cycle(productoA)
#cycle(productoB)
#cycle(productoC)
```

## Descomposición de las series

Donde:
'trend' se calcula con una media móvil.
'seasonal' se calcula promediando los valores de cada unidad de tiempo para todos los meses y centrando el resultado.
'random' se obtiene restando a la serie observada las dos componentes anteriores.

```{r}
productoA.ts.desc = decompose(productoA); plot(productoA.ts.desc,xlab = "Año")

productoB.ts.desc = decompose(productoB); plot(productoB.ts.desc,xlab = "Año")

productoC.ts.desc = decompose(productoC); plot(productoC.ts.desc,xlab = "Año")
```

### Gráficas desestacionadas

Si eliminamos la componente estacional y dejamos la componente de tendencia y el ruidp, obtenemos:

```{r}

productoA.des = productoA-productoA.ts.desc$seasonal
plot(productoA.des,main="Año Producto A")
productoB.des = productoB-productoB.ts.desc$seasonal
plot(productoB.des,main="Año Producto B")
productoC.des = productoC-productoC.ts.desc$seasonal
plot(productoC.des,main="Año Producto C")
```

## Transformación de la serie original

### 1. Estabilización de la varianza

```{r}
plot(log(productoA),main="Producto A")
plot(log(productoB),main="Producto B")
plot(log(productoC),main="Producto C")
```

### 2. Eliminación de tendencia

Para esto diferenciamos la serie; o sea, consideramos la serie de diferencias entre una observación y la anterior en lugar de la serie original.

```{r}
x = log(productoA); difA.x = diff(x); plot(difA.x)
x = log(productoB); difB.x = diff(x); plot(difB.x)
x = log(productoC); difC.x = diff(x); plot(difC.x)
```

### 3. Eliminación de estacionalidad

Para eliminar la estacionalidad de una serie mensual se pueden tomar diferencias estacionales de orden 12, como se muestra a continuación:

```{r}
difA2.difA.x = diff(difA.x,lag=12); plot(difA2.difA.x)
difB2.difB.x = diff(difB.x,lag=12); plot(difB2.difB.x)
difC2.difC.x = diff(difC.x,lag=12); plot(difC2.difC.x)
```

## Funciones de autocovarianzas y de autocorrelaciones

Representación del correlograma de la serie:

```{r}
# FUNCIONES DE AUTOCOVARIANZAS Y DE AUTOCORRELACIONES
yA = difA2.difA.x; acf(yA)
yB = difB2.difB.x; acf(yB)
yC = difC2.difC.x; acf(yC)
```

Donde las líneas punteadas representan las bandas de confianza, bajo la hipótesis de que la serie es un ruido blanco(no correlacionada).

### Valores numéricos de las correlaciones estimadas:

```{r}
# Valores numéricos
acf(yA,plot=FALSE)$acf
acf(yB,plot=FALSE)$acf
acf(yC,plot=FALSE)$acf
```

# PRONÓSTICO

Para realizar un pronóstico, decidimos utilizar el modelo estadístico ARIMA. Al no contar con valores p, d,q utilizamos auto.arima(), que nos brinda el mejor modelo a utilizar.

```{r}
auto.arima(productoA,trace=T) # (0,0,0)
auto.arima(productoB,trace=T) # (1,1,0)
auto.arima(productoC,trace=T) # (1,0,0)
```


```{r}
arimaA <- arima(productoA,order=c(0,0,0));
arimaB <- arima(productoB,order=c(1,1,0)); 
arimaC <- arima(productoC,order=c(1,0,0)); 
```

## Predicción

```{r include=FALSE}
productoA.forecast <- predict(arimaA,n.ahead=5); 

productoB.forecast <- predict(arimaB,n.ahead=5); 

productoC.forecast <- predict(arimaC,n.ahead=2);
```


## Suavizamiento y pronóstico

```{r}
library(smooth)
pA.2 <- sma(productoA,order=4,h=4,silent=FALSE)
pA <- forecast(pA.2); plot(pA,main="Producto A")

pB.2 <- sma(productoB,order=4,h=4,silent=FALSE)
pB <- forecast(pB.2); plot(pB,main="Producto B")

pC.2 <- sma(productoC,order=4,h=4,silent=FALSE)
pC <- forecast(pC.2); plot(pC,main="Producto C")
```





